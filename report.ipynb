{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4fb6b51",
   "metadata": {},
   "source": [
    "# Conversational Finance\n",
    "\n",
    "This dataset is composed of complicated financial questions, answerable with reference to tables and textual context. Either follow through the executed code in this notebook, or execute the code yourself to have a play around with the outputs of the models. Run the below cell once only to download the dataset, then comment it out again to prevent errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da09c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/czyssrs/ConvFinQA.git\n",
    "# !unzip ConvFinQA/data.zip -d ConvFinQA/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e87059",
   "metadata": {},
   "source": [
    "## Section 1: Dataset analysis\n",
    "\n",
    "Let's first preprocess the data and look at it in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "212416f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.preprocessing import Preprocessor\n",
    "\n",
    "with open(\"ConvFinQA/data/train.json\", \"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "preprocessor = Preprocessor(raw_data)\n",
    "data = preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd3aee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Example datapoint:\n",
      "  Question: what was the percentage change in the non interest income from from 2011 to 2012\n",
      "  Answer: 0.05357\n",
      "  Predicted answer: Not yet predicted\n",
      "  Table: +---------------------------------------------+-----------------------+-----------------------+\n",
      "|   year ended december 31dollars in millions | net interest income   | net interest margin   |\n",
      "+=============================================+=======================+=======================+\n",
      "|                                        2012 | $ 9640                | 3.94% ( 3.94 % )      |\n",
      "+---------------------------------------------+-----------------------+-----------------------+\n",
      "|                                        2011 | $ 8700                | 3.92% ( 3.92 % )      |\n",
      "+---------------------------------------------+-----------------------+-----------------------+\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# A random question\n",
    "\n",
    "import random\n",
    "\n",
    "from utils.general import print_example\n",
    "\n",
    "print_example(data[random.randint(0, len(data) - 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea51be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 3965\n",
      "Answer type counts: Counter({<class 'float'>: 3921, <class 'str'>: 44})\n",
      "Possible string answers: {'yes', 'no'}\n",
      "Example float answer: 0.14136\n"
     ]
    }
   ],
   "source": [
    "# Aggregate Dataset statistics\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print(f\"Dataset length: {len(data)}\")\n",
    "counter = Counter([type(d.answer) for d in data])\n",
    "print(f\"Answer type counts: {counter}\")\n",
    "print(f\"Possible string answers: {set([d.answer for d in data if isinstance(d.answer, str)])}\")\n",
    "print(f\"Example float answer: {data[0].answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec67e218",
   "metadata": {},
   "source": [
    "The vast majority of the examples in the data have floats as answers. I will constrain this investigation into question answering for these as opposed to the minority of 'yes/no' questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a41a95f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length after filtering: 3921\n"
     ]
    }
   ],
   "source": [
    "data = [d for d in data if isinstance(d.answer, float)]\n",
    "\n",
    "print(f\"Dataset length after filtering: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f2e90b",
   "metadata": {},
   "source": [
    "With this filtering done, I want to go through three different kinds of example I saw:\n",
    "\n",
    "1. Good context, good answer (Ideal)\n",
    "2. Bad context, good answer (Underinformed)\n",
    "3. Good context, bad answer (Incorrectly labelled)\n",
    "\n",
    "### Ideal\n",
    "\n",
    "In the below example, we can see a question, answer, and the table used to answer the question. (Note that there is also context I have chosen not to print out for the sake of focus.) This kind of example is the *ideal* type within our dataset: the required data is present in the table and the gold standard answer is correct. We can generate an answer to this question and compare it to a gold standard to evaluate the quality of our system effectively. A comparison between a predicted answer and gold standard answer will be *meaningful*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7630642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Example datapoint:\n",
      "  Question: what was the percent of the growth in the revenues from 2007 to 2008\n",
      "  Answer: 0.01269\n",
      "  Predicted answer: Not yet predicted\n",
      "  Table: +-------------------------------------------+-----------+----------------------------------------------------------------------+----------------------------+------------------------------+\n",
      "|                                           | revenue   |   income from continuing operations available to common stockholders |   basic earnings per share |   diluted earnings per share |\n",
      "+===========================================+===========+======================================================================+============================+==============================+\n",
      "| year ended december 31 2008 ( unaudited ) | $ 9362.2  |                                                                285.7 |                       0.76 |                         0.75 |\n",
      "+-------------------------------------------+-----------+----------------------------------------------------------------------+----------------------------+------------------------------+\n",
      "| year ended december 31 2007 ( unaudited ) | $ 9244.9  |                                                                423.2 |                       1.1  |                         1.09 |\n",
      "+-------------------------------------------+-----------+----------------------------------------------------------------------+----------------------------+------------------------------+\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print_example(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7687a71-0510-4730-b86c-48941c0cc5a6",
   "metadata": {},
   "source": [
    "### Underinformed\n",
    "\n",
    "On the other hand, consider the example shown below. The data is corrupted in some way, such that two rows are marked as being the 'year ended june 30 2009 2008'. It is not really clear how we should answer this question. I take the view that, in fact, in a production question-answering system, we **should not** answer this question, but request clarification or data cleaning from the end-user. For this initial analysis application, I have chosen to output a flag, **CANNOT ANSWER**, which could be picked up in an end-to-end chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "940b7d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Example datapoint:\n",
      "  Question: what was the percentage change in the net cash from operating activities from 2008 to 2009\n",
      "  Answer: 0.14136\n",
      "  Predicted answer: Not yet predicted\n",
      "  Table: +------------------------------+--------------+---------------------+-------------------------+------------------------------+------------------------------------------+--------------------------------------+\n",
      "| 2008                         | net income   |   non-cash expenses | change in receivables   |   change in deferred revenue | change in other assets and liabilities   | net cash from operating activities   |\n",
      "+==============================+==============+=====================+=========================+==============================+==========================================+======================================+\n",
      "| year ended june 30 2009 2008 | $ 103102     |               74397 | 21214                   |                        21943 | -14068 ( 14068 )                         | $ 206588                             |\n",
      "+------------------------------+--------------+---------------------+-------------------------+------------------------------+------------------------------------------+--------------------------------------+\n",
      "| year ended june 30 2009 2008 | $ 104222     |               70420 | -2913 ( 2913 )          |                         5100 | 4172                                     | $ 181001                             |\n",
      "+------------------------------+--------------+---------------------+-------------------------+------------------------------+------------------------------------------+--------------------------------------+\n",
      "| year ended june 30 2009      | $ 104681     |               56348 | -28853 ( 28853 )        |                        24576 | 17495                                    | $ 174247                             |\n",
      "+------------------------------+--------------+---------------------+-------------------------+------------------------------+------------------------------------------+--------------------------------------+\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print_example(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29da5783-3c87-49fb-b612-0a9b641473fb",
   "metadata": {},
   "source": [
    "### Incorrectly labelled\n",
    "\n",
    "In the below example, data required to answer the question is present but the gold standard label is incorrect. In this case, the portion of the total shares subject to outstanding awards is *clearly* 2530454 / (2530454 + 5923147) = ~0.29, not the gold standard answer which puts the 2004 shares in the numerator. These questions are dangerous in our dataset. In fact, they represent an automatic performance penalty to any hypothetically perfect model. Any satisfactory analysis should take care that mismatches between predicted answers and incorrectly labelled answers are, at the very least, not marking down system performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c65be23-0c43-4e73-8a2b-f79609f22feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Example datapoint:\n",
      "  Question: what portion of the total shares subject to outstanding awards is under the 2009 global incentive plan?\n",
      "  Answer: 0.70067\n",
      "  Predicted answer: Not yet predicted\n",
      "  Table: +--------------------------------------+------------------------------+-----------------------------+\n",
      "|                                      |   2009 global incentive plan | 2004 stock incentive plan   |\n",
      "+======================================+==============================+=============================+\n",
      "| shares available for awards          |                      2322450 | -                           |\n",
      "+--------------------------------------+------------------------------+-----------------------------+\n",
      "| shares subject to outstanding awards |                      2530454 | 5923147                     |\n",
      "+--------------------------------------+------------------------------+-----------------------------+\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print_example(data[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7881f7-6e2c-4834-b7da-96f511b1c228",
   "metadata": {},
   "source": [
    "### Section 1 Summary\n",
    "\n",
    "We have different sources of data which we need to handle. We should primarily measure system accuracy using *ideal* questions, not *underinformed* or *incorrectly labelled*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f32b7c",
   "metadata": {},
   "source": [
    "## The implementation\n",
    "\n",
    "This dataset is highly amenable to usage with LLMs. Each question requires combining some context and domain knowledge to answer. Because the context is fairly small, we don't need to do any context retrieval to derive the answer to each question. The information needed to answer them is all included in relatively short windows of text and structured tables. If we structure this information the right way, and provide it to a sufficiently powerful LLM with enough domain knowledge, it should be able to answer the questions (at least the ideal ones).\n",
    "\n",
    "## Baseline build\n",
    "\n",
    "The baseline was simply a model instructed to directly answer the question, provided the table and context. This model is linked below, and resulted in initial accuracies of around 40%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e35834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY=\"\"\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a098fa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Example datapoint:\n",
      "  Question: what was the percentage change in net sales from 2000 to 2001?\n",
      "  Answer: -0.3282\n",
      "  Predicted answer: -32.81\n",
      "  Table: +------+-------------+-----------------+----------------+---------------------------+\n",
      "|      | net sales   |   cost of sales | gross margin   | gross margin percentage   |\n",
      "+======+=============+=================+================+===========================+\n",
      "| 2002 | $ 5742      |            4139 | $ 1603         | 28% ( 28 % )              |\n",
      "+------+-------------+-----------------+----------------+---------------------------+\n",
      "| 2001 | $ 5363      |            4128 | $ 1235         | 23% ( 23 % )              |\n",
      "+------+-------------+-----------------+----------------+---------------------------+\n",
      "| 2000 | $ 7983      |            5817 | $ 2166         | 27% ( 27 % )              |\n",
      "+------+-------------+-----------------+----------------+---------------------------+\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from agents.question_answering import QuestionAnswerer\n",
    "from agents.prompts.question_answering import first_pass\n",
    "\n",
    "first_pass_agent = QuestionAnswerer(client=client, model='gpt-4o', system_prompt=first_pass.system_prompt, user_prompt=first_pass.user_prompt)\n",
    "answered_example = first_pass_agent.predict(data[2])\n",
    "\n",
    "print_example(answered_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb691a2",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "\n",
    "To improve the model, I sectioned off my data into a training set, which could be experimented with and used to optimise the model, and a validation set, which would only be run at the very end of optimisation to ensure I was not overfitting the model to the dataset. The main changes are listed below.\n",
    "\n",
    "### Model output change\n",
    "\n",
    "A large issue with the baseline system is that while LLMs are excellent retrievers and organisers of information, LLMs frequently make mathematical errors, particularly when generating precise floating point representations of numbers. This creates a source of mismatch between answers.\n",
    "\n",
    "Given these results, and inspired by the original method pursued in the paper, I asked the model to simply retrieve the necessary numbers and output a mathematical expression which could be extracted and computed precisely after retrieval. This yielded much better results.\n",
    "\n",
    "### Standardised units\n",
    "\n",
    "The outputs in the gold standard can be a bit inconsistent. Most of the time, however, they follow a pattern. For example, percentages are usually referenced in decimals (although occasionally are represented as integer values). I attempt to follow the rough standard within the dataset, but will also do some fuzzy evaluation later on to account for the inconsistency in the training data.\n",
    "\n",
    "### Applying correct formulas\n",
    "\n",
    "I accounted for a standard error type, such as when calculating rates of growth, that I saw during optimisation. If I was to continue this optimisation, I would find other common errors and also add some prompting to guide the model towards the correct behaviour over time.\n",
    "\n",
    "To see these optimisations in the prompt, uncomment and run the below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6537e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from agents.prompts.question_answering.final import system_prompt\n",
    "\n",
    "# print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd71f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, validation_set = data[:-1000], data[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "95804686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Example datapoint:\n",
      "  Question: what was the percentage change in net sales from 2000 to 2001?\n",
      "  Answer: -0.3282\n",
      "  Predicted answer: -0.3281974195164725\n",
      "  Table: +------+-------------+-----------------+----------------+---------------------------+\n",
      "|      | net sales   |   cost of sales | gross margin   | gross margin percentage   |\n",
      "+======+=============+=================+================+===========================+\n",
      "| 2002 | $ 5742      |            4139 | $ 1603         | 28% ( 28 % )              |\n",
      "+------+-------------+-----------------+----------------+---------------------------+\n",
      "| 2001 | $ 5363      |            4128 | $ 1235         | 23% ( 23 % )              |\n",
      "+------+-------------+-----------------+----------------+---------------------------+\n",
      "| 2000 | $ 7983      |            5817 | $ 2166         | 27% ( 27 % )              |\n",
      "+------+-------------+-----------------+----------------+---------------------------+\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "question_answerer = QuestionAnswerer(client=client, model='gpt-4o')\n",
    "answered_example = question_answerer.predict(data[2])\n",
    "\n",
    "print_example(answered_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "286be3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold standard answer: -0.3282\n",
      "Predicted answer: -0.3281974195164725\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gold standard answer: {answered_example.answer}\\nPredicted answer: {answered_example.predicted_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aea6aa1",
   "metadata": {},
   "source": [
    "As you can see these answers are now indeed the same, with the exception of a rounding issue. I define an evaluator class to take account of this, by rounding to the gold standard answer's level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2dbc8557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator()\n",
    "evaluator.accuracy([data[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4e2458",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "While we are already handling rounding errors, there are a few other common mismatches between the gold standard output and the predicted output. It is often the case, for example, that the outputs are represented in different unit sizes, even accounting for overall trends. \n",
    "\n",
    "For example, if in the context amounts of money are referred to in the millions (such that 3 then refers to 3 million) the gold standard label could possibly be represented in the condensed format or its full form (3 or 3000000). Because of this, sometimes the model's output will misalign with the gold standard label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f8e00a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length after filtering: 1041\n",
      "Dataset accuracy: 0.7435158501440923\n",
      "Incorrect examples: 243\n"
     ]
    }
   ],
   "source": [
    "# I use some data I already passed through the model here, but run it with the final model if you'd like\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"pregenerated_examples/training_examples_3.pkl\", \"rb\") as f:\n",
    "    pre_annotated_data = pickle.load(f)\n",
    "\n",
    "pre_annotated_data = [\n",
    "    d for d in pre_annotated_data if isinstance(d.answer, float)\n",
    "]\n",
    "\n",
    "pre_annotated_data = [d for d in pre_annotated_data if d.predicted_answer is not None]\n",
    "\n",
    "print(f\"Dataset length after filtering: {len(pre_annotated_data)}\")\n",
    "print(f\"Dataset accuracy: {evaluator.accuracy(pre_annotated_data)}\")\n",
    "\n",
    "incorrect_examples = evaluator.return_incorrect(pre_annotated_data)\n",
    "print(f\"Incorrect examples: {len(incorrect_examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46ae2bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Example datapoint:\n",
      "  Question: what is the percentual decrease observed in the future minimum rental payments during 2008 and 2009?\n",
      "  Answer: -0.13249\n",
      "  Predicted answer: 0.13249211356466878\n",
      "  Table: +--------+--------+--------+--------+--------+---------------+-----------------------------------+\n",
      "| 2008   |   2009 |   2010 |   2011 |   2012 |   later years | total minimum payments required   |\n",
      "+========+========+========+========+========+===============+===================================+\n",
      "| $ 317  |    275 |    236 |    214 |    191 |           597 | $ 1830                            |\n",
      "+--------+--------+--------+--------+--------+---------------+-----------------------------------+\n",
      "  \n",
      "\n",
      "  Example datapoint:\n",
      "  Question: what percent did the realized and unrealized losses effect the assets as of 2008?\n",
      "  Answer: 0.3347\n",
      "  Predicted answer: -0.4846066134549601\n",
      "  Table: +--------------+--------------------+--------------------------------------------------+-------------------------------------------------------+------------------------------------------+--------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|              | december 31 2007   | realized and unrealized gains / ( losses ) net   |   purchases sales other settlements and issuances net | net transfers in and/or out of level 3   | december 31 2008   | total net ( losses ) for the period included in earnings attributable to the change in unrealized gains or ( losses ) relating to assets stillheld at the reporting date   |\n",
      "+==============+====================+==================================================+=======================================================+==========================================+====================+============================================================================================================================================================================+\n",
      "| investments  | $ 1240             | -409 ( 409 )                                     |                                                    11 | -29 ( 29 )                               | $ 813              | $ -366 ( 366 )                                                                                                                                                             |\n",
      "+--------------+--------------------+--------------------------------------------------+-------------------------------------------------------+------------------------------------------+--------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| other assets | $ 2014             | -16 ( 16 )                                       |                                                     2 | 78                                       | $ 64               | $ -17 ( 17 )                                                                                                                                                               |\n",
      "+--------------+--------------------+--------------------------------------------------+-------------------------------------------------------+------------------------------------------+--------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "  \n",
      "\n",
      "  Example datapoint:\n",
      "  Question: by how much did allowance for other funds used during construction increase from 2016 to 2018?\n",
      "  Answer: 0.6\n",
      "  Predicted answer: 9\n",
      "  Table: +------+------------------------------------------------------+---------------------------------------------------------+\n",
      "|      | allowance for other funds used during construction   |   allowance for borrowed funds used during construction |\n",
      "+======+======================================================+=========================================================+\n",
      "| 2018 | $ 24                                                 |                                                      13 |\n",
      "+------+------------------------------------------------------+---------------------------------------------------------+\n",
      "| 2017 | $ 19                                                 |                                                       8 |\n",
      "+------+------------------------------------------------------+---------------------------------------------------------+\n",
      "| 2016 | $ 15                                                 |                                                       6 |\n",
      "+------+------------------------------------------------------+---------------------------------------------------------+\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "errors = [incorrect_examples[i] for i in [6, 15, 44]]\n",
    "\n",
    "for example in errors:\n",
    "    print_example(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ca11e",
   "metadata": {},
   "source": [
    "What do these errors show? They show a difference in output format, where the calculated answer is not the same. \n",
    "\n",
    "The first shows a difference in sign. Because the question already implied a downward shift, the predicted answer leaves out the negative sign. This seems not like an error.\n",
    "\n",
    "The second shows a difference in unit. The answer is requested in millions, but the question was answered in billions. This is a legitimate error.\n",
    "\n",
    "The third shows a difference in representing percentages. Here, the gold standard answer bucks the trend throughout the rest of the dataset of representing percentages as decimals. This does not feel like an error.\n",
    "\n",
    "Because of the inconsistency in the dataset and the predicted answers I propose an alternative metric whereby we 'fuzzily' match answers and ignore these differences in scale and direction. This will result in a metric that captures if we've done the maths right. This metric will also, however, leave open the possibility that the answer is formatted in the wrong way - as can be seen in the second example.\n",
    "\n",
    "This results in a score which tends to go up about 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b9328437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-fuzzy (strict) matching: 0.7435158501440923\n",
      "Fuzzy (permissive) matching: 0.8001921229586936\n"
     ]
    }
   ],
   "source": [
    "print(f\"Non-fuzzy (strict) matching: {evaluator.accuracy(pre_annotated_data, fuzzy=False)}\")\n",
    "print(f\"Fuzzy (permissive) matching: {evaluator.accuracy(pre_annotated_data, fuzzy=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db5bc2",
   "metadata": {},
   "source": [
    "This leaves the 20% of examples where the predicted and gold standard answer *do not match at all*. The immediate conclusion to jump to is that 20% of our predictions are wrong. However, recall our inaccurately labeled examples. If our model correctly predicts the answer for these examples the model will be marked as incorrect. Indeed, see the example cited during our data analysis, where indeed our model predicts the correct result creating a mismatch with the incorrect label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4880b8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Example datapoint:\n",
      "  Question: what portion of the total shares subject to outstanding awards is under the 2009 global incentive plan?\n",
      "  Answer: 0.70067\n",
      "  Predicted answer: 0.2993344493074608\n",
      "  Table: +--------------------------------------+------------------------------+-----------------------------+\n",
      "|                                      |   2009 global incentive plan | 2004 stock incentive plan   |\n",
      "+======================================+==============================+=============================+\n",
      "| shares available for awards          |                      2322450 | -                           |\n",
      "+--------------------------------------+------------------------------+-----------------------------+\n",
      "| shares subject to outstanding awards |                      2530454 | 5923147                     |\n",
      "+--------------------------------------+------------------------------+-----------------------------+\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "question_answerer.predict(data[6])\n",
    "print_example(data[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8550ba45",
   "metadata": {},
   "source": [
    "### Answer Checking\n",
    "\n",
    "I've defined an LLM agent which has the same available information and then compares the gold standard answer with the predicted answer, and chooses a preferred answer (while explaining it's reasoning). I use a different LLM to do this (DeepSeek's V3 model) so that OpenAI isn't 'marking its own homework'. This relies on LLMs' ability to alter their reasoning when they have the correct answer provided to them. We can then use this to get a sense of how often our predicted answers are actually wrong.\n",
    "\n",
    "Of course, this is a risky method of evaluation. It is completely possible that *neither* the gold standard nor the predicted answer are correct, or that the answer checker makes the wrong evaluation. This metric might therefore be used best to establish a potential 'performance ceiling' or 'band' in which the true performance might exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c32b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.answer_checker import AnswerChecker\n",
    "\n",
    "DEEPSEEK_API_KEY = \"\"\n",
    "\n",
    "deep_seek_client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "answer_checker = AnswerChecker(deep_seek_client, model='deepseek-chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6889495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment this to run the answer checker on the incorrect examples\n",
    "\n",
    "# from collections import Counter\n",
    "\n",
    "# for eg in tqdm(incorrect_examples):\n",
    "#     answer_checker.predict(eg)\n",
    "\n",
    "# count = Counter([d.preferred_answer for d in incorrec t_examples])\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b494ceb4-06b1-408c-807e-1e5df8e3ac3f",
   "metadata": {},
   "source": [
    "## Validation Evaluation\n",
    "\n",
    "I have been working with approximately the first 2000 rows of train.json. For the purposes of this experiment, I will use the last 1000 rows as a held out, final evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4c4745ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set length: 996\n"
     ]
    }
   ],
   "source": [
    "# for i in tqdm(range(len(validation_set))):\n",
    "#     question_answerer.predict(validation_set[i])\n",
    "\n",
    "with open(\"pregenerated_examples/validation_set.pkl\", \"rb\") as f: # just load them in if you don't want to wait a few hours\n",
    "    validation_set = pickle.load(f)\n",
    "\n",
    "validation_set = [d for d in validation_set if isinstance(d.answer, float)]\n",
    "\n",
    "for i in range(len(validation_set)):\n",
    "    if validation_set[i].predicted_answer is None:\n",
    "        validation_set[i].predicted_answer = \"CANNOT ANSWER\"\n",
    "    \n",
    "\n",
    "print(f\"Validation set length: {len(validation_set)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2699ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7339357429718876\n",
      "0.8012048192771084\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.accuracy(validation_set, fuzzy=False))\n",
    "print(evaluator.accuracy(validation_set, fuzzy=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bd66fb44-aabf-4c18-9982-a5874108f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_validation_examples = evaluator.return_incorrect(validation_set)\n",
    "\n",
    "# for example in tqdm(incorrect_validation_examples):\n",
    "#     answer_checker.predict(example)\n",
    "\n",
    "with open(\"pregenerated_examples/incorrect_validation_examples.pkl\", \"rb\") as f: # just load them in if you don't want to wait a few hours\n",
    "    incorrect_validation_examples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "94c0b32b-bc85-484c-8c4f-309ccf472a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'predicted': 159, 'gold': 95})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "count = Counter([d.preferred_answer for d in incorrect_validation_examples])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5122a338-2e3f-42f6-bf6a-ac29cb5cb11e",
   "metadata": {},
   "source": [
    "By our most fuzzy metric, that is, if we assume that all of the times our evaluator preferred the predicted answer over the generated answer alongside our other fuzzing (accounting for mismatching signs, and different scales), the model is over 90% accurate.\n",
    "\n",
    "It should be said that this is unlikely to be the true measure of performance. A more conservative performance estimate might be made by taking out the examples marked by the answer checker as favouring the predicted answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dbac7e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most generous accuracy: 90.46%\n",
      "Accuracy removing ambiguous cases: 88.65%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Most generous accuracy: {(1 - (count['gold']/len(validation_set))) * 100:.2f}%\")\n",
    "print(f\"Accuracy removing ambiguous cases: {(1 - (count['gold']/(len(validation_set) - count['predicted']))) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f2dd2a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "These results show a wide band of possible performance from our model. Around 75% of answers are exactly answered with this application. By being more flexible in this, accounting for variation in how the questions are answered, 80% are answered. Attempting to remove for incorrectly labelled examples, we saw total possible accuracy as being 90.39%, or by removing ambiguous examples we see fuzzy accuracy as 88.52%. \n",
    "\n",
    "For future work, the 10% of gold standard answers that the Answer Checker preferred potentially represent the most challenging, interesting examples that future projects need to optimise for. Additionally I decided for the purposes of this project that simply *not answering* (in the case where the model decided the context was too uninformative) was not an incorrect response or a correct response. Future possible investigations should evaluate the decisions to not answer, and see whether they were legitimate. After all, they could just represent very hard, interesting questions that require more domain knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ea71d3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
